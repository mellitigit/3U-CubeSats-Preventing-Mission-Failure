{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Battery Prediction Model - GRU Alternative\n",
        "\n",
        "This notebook trains a **GRU (Gated Recurrent Unit)** model as an alternative to the LSTM model. GRUs are simpler and faster than LSTMs while often achieving similar performance.\n",
        "\n",
        "## Model Architecture Comparison:\n",
        "- **LSTM**: 3 gates (input, forget, output) - more parameters\n",
        "- **GRU**: 2 gates (update, reset) - fewer parameters, faster training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "df = pd.read_csv('synthetic_battery_prediction_data.csv')\n",
        "\n",
        "feature_cols = ['temperature', 'voltage', 'current', 'power', 'delta_temp', 'delta_voltage', 'delta_current']\n",
        "target_cols = ['temperature_next', 'voltage_next', 'current_next']\n",
        "\n",
        "# Scale features\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "X_scaled = scaler_X.fit_transform(df[feature_cols])\n",
        "y_scaled = scaler_y.fit_transform(df[target_cols])\n",
        "\n",
        "print(f\"Data shape: {X_scaled.shape}\")\n",
        "print(f\"Features: {feature_cols}\")\n",
        "print(f\"Targets: {target_cols}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sequences\n",
        "seq_length = 10\n",
        "\n",
        "def create_sequences(X, y, seq_length):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X) - seq_length):\n",
        "        X_seq.append(X[i:i+seq_length])\n",
        "        y_seq.append(y[i+seq_length])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "X_seq, y_seq = create_sequences(X_scaled, y_scaled, seq_length)\n",
        "print(f\"Sequence shape: {X_seq.shape}\")\n",
        "print(f\"Target shape: {y_seq.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into train/validation/test (60/20/20)\n",
        "X_tensor = torch.tensor(X_seq, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y_seq, dtype=torch.float32)\n",
        "\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.6 * total_size)\n",
        "val_size = int(0.2 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    dataset, [train_size, val_size, test_size]\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRU Model Definition with Stronger Regularization\n",
        "class BatteryGRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.3, bidirectional=False):\n",
        "        super(BatteryGRU, self).__init__()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.gru = nn.GRU(\n",
        "            input_size, \n",
        "            hidden_size, \n",
        "            num_layers, \n",
        "            batch_first=True, \n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "        # If bidirectional, hidden size is doubled\n",
        "        gru_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
        "        # Multiple dropout layers for stronger regularization\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        # Optional: Add a hidden layer with dropout for more regularization\n",
        "        self.fc1 = nn.Linear(gru_output_size, gru_output_size // 2)\n",
        "        self.dropout2 = nn.Dropout(dropout * 0.8)  # Slightly less dropout in second layer\n",
        "        self.fc2 = nn.Linear(gru_output_size // 2, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)          # out: [batch, seq_len, hidden]\n",
        "        out = out[:, -1, :]            # take last timestep\n",
        "        out = self.dropout1(out)       # First dropout\n",
        "        out = self.fc1(out)            # First FC layer\n",
        "        out = torch.relu(out)          # Activation\n",
        "        out = self.dropout2(out)       # Second dropout\n",
        "        out = self.fc2(out)            # Final output layer\n",
        "        return out\n",
        "\n",
        "input_size = X_seq.shape[2]\n",
        "# Reduced model size to prevent overfitting\n",
        "hidden_size = 48  # Reduced from 64\n",
        "num_layers = 2\n",
        "output_size = y_seq.shape[1]\n",
        "dropout_rate = 0.35  # Increased from 0.2 for stronger regularization\n",
        "\n",
        "# Create GRU model (can switch to bidirectional=True for Bidirectional GRU)\n",
        "model = BatteryGRU(input_size, hidden_size, num_layers, output_size, dropout=dropout_rate, bidirectional=False)\n",
        "\n",
        "print(f\"Model: GRU (with stronger regularization)\")\n",
        "print(f\"Input size: {input_size}, Hidden size: {hidden_size}, Layers: {num_layers}\")\n",
        "print(f\"Dropout rate: {dropout_rate}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training setup with STRONGER anti-overfitting techniques\n",
        "criterion = nn.MSELoss()\n",
        "# Increased weight decay significantly for stronger L2 regularization\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0008, weight_decay=1e-3)  # Increased from 1e-5 to 1e-3\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6)\n",
        "\n",
        "num_epochs = 50\n",
        "patience = 15  # More patience to allow model to train longer\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Gradient clipping to prevent exploding gradients\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "print(\"Starting GRU training with stronger regularization...\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Weight decay: {optimizer.param_groups[0]['weight_decay']}\")\n",
        "print(f\"Initial learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "print(f\"Gradient clipping: {max_grad_norm}\")\n",
        "print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop with gradient clipping\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(xb)\n",
        "        loss = criterion(outputs, yb)\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient clipping to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        \n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    \n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    \n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            outputs = model(xb)\n",
        "            loss = criterion(outputs, yb)\n",
        "            val_loss += loss.item()\n",
        "    \n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    \n",
        "    scheduler.step(avg_val_loss)\n",
        "    \n",
        "    # Early stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), 'best_gru_model.pth')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    \n",
        "    # Calculate gap percentage for monitoring\n",
        "    gap_percent = ((avg_val_loss - avg_train_loss) / avg_train_loss) * 100 if avg_train_loss > 0 else 0\n",
        "    \n",
        "    print(f\"Epoch {epoch+1:3d}/{num_epochs} | Train: {avg_train_loss:.6f} | Val: {avg_val_loss:.6f} | \"\n",
        "          f\"Gap: {gap_percent:.1f}% | LR: {optimizer.param_groups[0]['lr']:.6f}\", end=\"\")\n",
        "    print(\" *\" if patience_counter == 0 else f\" (patience: {patience_counter}/{patience})\")\n",
        "    \n",
        "    if patience_counter >= patience:\n",
        "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
        "        print(f\"Loading best model with val loss: {best_val_loss:.6f}\")\n",
        "        model.load_state_dict(torch.load('best_gru_model.pth'))\n",
        "        break\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot loss curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(train_losses, label='Training Loss', linewidth=2)\n",
        "plt.plot(val_losses, label='Validation Loss', linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.title('GRU Model - Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.yscale('log')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Check for overfitting\n",
        "final_train_loss = train_losses[-1]\n",
        "final_val_loss = val_losses[-1]\n",
        "gap_percent = ((final_val_loss - final_train_loss) / final_train_loss) * 100 if final_train_loss > 0 else 0\n",
        "\n",
        "print(f\"\\nFinal Training Loss: {final_train_loss:.6f}\")\n",
        "print(f\"Final Validation Loss: {final_val_loss:.6f}\")\n",
        "print(f\"Gap: {gap_percent:.2f}%\")\n",
        "print(\"✓ Good generalization\" if gap_percent < 10 else \"⚠️ Possible overfitting\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "model.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        outputs = model(xb)\n",
        "        predictions.append(outputs.numpy())\n",
        "        actuals.append(yb.numpy())\n",
        "\n",
        "predictions = np.vstack(predictions)\n",
        "actuals = np.vstack(actuals)\n",
        "\n",
        "# Inverse scale\n",
        "predictions_orig = scaler_y.inverse_transform(predictions)\n",
        "actuals_orig = scaler_y.inverse_transform(actuals)\n",
        "\n",
        "# Calculate metrics\n",
        "rmse = np.sqrt(mean_squared_error(actuals_orig, predictions_orig, multioutput='raw_values'))\n",
        "mae = mean_absolute_error(actuals_orig, predictions_orig, multioutput='raw_values')\n",
        "r2 = r2_score(actuals_orig, predictions_orig, multioutput='raw_values')\n",
        "\n",
        "print(\"GRU Model Performance:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Temperature - RMSE: {rmse[0]:.4f} °C, MAE: {mae[0]:.4f} °C, R²: {r2[0]:.4f}\")\n",
        "print(f\"Voltage     - RMSE: {rmse[1]:.6f} V,  MAE: {mae[1]:.6f} V,  R²: {r2[1]:.4f}\")\n",
        "print(f\"Current     - RMSE: {rmse[2]:.6f} A,  MAE: {mae[2]:.6f} A,  R²: {r2[2]:.4f}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization\n",
        "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
        "targets = ['Temperature', 'Voltage', 'Current']\n",
        "units = ['°C', 'V', 'A']\n",
        "colors = ['red', 'blue', 'green']\n",
        "\n",
        "for idx, (target, unit, color) in enumerate(zip(targets, units, colors)):\n",
        "    axes[idx].plot(actuals_orig[:, idx], label=f'{target} Actual', alpha=0.7, color=color, linewidth=1.5)\n",
        "    axes[idx].plot(predictions_orig[:, idx], label=f'{target} Predicted', alpha=0.7, color=color, linestyle='--', linewidth=1.5)\n",
        "    axes[idx].set_xlabel('Time step')\n",
        "    axes[idx].set_ylabel(f'{target} ({unit})')\n",
        "    axes[idx].set_title(f'GRU Model - {target} Prediction vs Actual')\n",
        "    axes[idx].legend()\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model and scalers\n",
        "torch.save(model.state_dict(), 'battery_gru_model.pth')\n",
        "with open('scaler_X_gru.pkl', 'wb') as f:\n",
        "    import pickle\n",
        "    pickle.dump(scaler_X, f)\n",
        "with open('scaler_y_gru.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler_y, f)\n",
        "\n",
        "model_params = {\n",
        "    'input_size': input_size,\n",
        "    'hidden_size': hidden_size,\n",
        "    'num_layers': num_layers,\n",
        "    'output_size': output_size,\n",
        "    'seq_length': seq_length,\n",
        "    'model_type': 'GRU'\n",
        "}\n",
        "with open('model_params_gru.pkl', 'wb') as f:\n",
        "    pickle.dump(model_params, f)\n",
        "\n",
        "print(\"Model and scalers saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment with Different Variants\n",
        "\n",
        "### Try Bidirectional GRU:\n",
        "Change line in Cell 5:\n",
        "```python\n",
        "model = BatteryGRU(..., bidirectional=True)  # Instead of False\n",
        "```\n",
        "\n",
        "### Try Different Hyperparameters:\n",
        "- `hidden_size = 128` (more capacity)\n",
        "- `num_layers = 3` (deeper network)\n",
        "- `dropout_rate = 0.3` (more regularization)\n",
        "- `seq_length = 15` or `20` (longer sequences)\n",
        "\n",
        "### Other Model Ideas:\n",
        "1. **CNN-LSTM Hybrid**: Use 1D CNN for feature extraction, then LSTM\n",
        "2. **Transformer**: Attention-based model (more complex)\n",
        "3. **Ensemble**: Combine predictions from LSTM and GRU models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Anti-Overfitting Improvements Applied:\n",
        "\n",
        "1. **Increased Dropout**: 0.2 → 0.35 (stronger regularization)\n",
        "2. **Increased Weight Decay**: 1e-5 → 1e-3 (stronger L2 regularization)\n",
        "3. **Reduced Model Size**: Hidden size 64 → 48 (fewer parameters)\n",
        "4. **Additional FC Layer**: Added hidden layer with dropout for more regularization\n",
        "5. **Gradient Clipping**: Prevents exploding gradients (max_norm=1.0)\n",
        "6. **Lower Learning Rate**: 0.001 → 0.0008 (more stable training)\n",
        "7. **More Patience**: Allows model to train longer before early stopping\n",
        "\n",
        "### If still overfitting, try:\n",
        "- Further reduce hidden_size to 32\n",
        "- Increase dropout to 0.4-0.5\n",
        "- Increase weight_decay to 1e-2\n",
        "- Reduce sequence length to 5-8\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
